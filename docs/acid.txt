-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Artificial Circuit Designer
--   
--   Please see the README on GitHub at
--   <a>https://github.com/augustunderground/acid#readme</a>
@package acid
@version 0.1.0.0


-- | Extensions to Torch
module Torch.Extensions

-- | Swaps the arguments of HaskTorch's foldLoop around
foldLoop' :: Int -> (a -> Int -> IO a) -> a -> IO a

-- | Because snake_case sucks
nanToNum :: Float -> Float -> Float -> Tensor -> Tensor

-- | Default limits for <a>nanToNum</a>
nanToNum' :: Tensor -> Tensor

-- | Default limits for <a>nanToNum</a> (0.0)
nanToNum'' :: Tensor -> Tensor

-- | GPU Tensor filled with Float value
fullLike' :: Tensor -> Float -> Tensor

-- | Select index with [Int] from GPU tensor
indexSelect'' :: Int -> [Int] -> Tensor -> Tensor

-- | Torch.where' with fixed type for where'
where'' :: Tensor -> (Tensor -> Tensor) -> Tensor -> Tensor

-- | Syntactic sugar for HaskTorch's <tt>repeatInterleave</tt> so it can
--   more easily be fmapped.
repeatInterleave' :: Int -> Tensor -> Tensor -> Tensor

-- | Helper function creating split indices as gpu int tensor splits' ::
--   [Int] -&gt; [T.Tensor] splits' = map tit . splits where tit i =
--   T.asTensor (i :: Int)
--   
--   Split Tensor into list of Tensors along dimension
splitDim :: Int -> Tensor -> [Tensor]

-- | Create Boolean Mask Tensor from list of indices.
boolMask :: Int -> [Int] -> Tensor

-- | Create a Boolean Mask Tensor from index Tensor
boolMask' :: Int -> Tensor -> Tensor

-- | GPU 1
gpu :: Device

-- | CPU 0
cpu :: Device

-- | Default Tensor Data Type
dataType :: DType

-- | Convert an Array to a Tensor on GPU
toTensor :: TensorLike a => a -> Tensor

-- | Convert an Array to a Tensor on CPU
toTensor' :: TensorLike a => a -> Tensor

-- | Convert an Array to a Tensor on GPU
toIntTensor :: TensorLike a => a -> Tensor

-- | Convert an Array to a Tensor on CPU
toIntTensor' :: TensorLike a => a -> Tensor

-- | Create an empty Float Tensor on GPU
empty :: Tensor

-- | Create an empty Float Tensor on CPU
empty' :: Tensor

-- | Convert a Scalar to a Tensor on GPU
toScalar :: Float -> Tensor

-- | Convert a Scalar to a Tensor on CPU
toScalar' :: Float -> Tensor

-- | Convert model to Double on GPU
toDouble :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Double on CPU
toDouble' :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Float on CPU
toFloat :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Float on CPU
toFloat' :: forall a. HasTypes a Tensor => a -> a

-- | Generate a Tensor of random Integers on GPU
randomInts :: Int -> Int -> Int -> IO Tensor

-- | Generate a Tensor of random Integers on CPU
randomInts' :: Int -> Int -> Int -> IO Tensor

-- | Generate Normally Distributed Random values given dimensions
normal' :: [Int] -> IO Tensor

-- | Generate Uniformally distributed values in a given range
uniform' :: [Int] -> Float -> Float -> IO Tensor

-- | Rescale tensor s.t. mean = 0.0 and std = 1.0
rescale :: Tensor -> Tensor


-- | Utility and Helper functions for EDELWACE
module Lib

-- | Run Mode
data Mode

-- | Start Agent Training
Train :: Mode

-- | Continue Agent Training
Continue :: Mode

-- | Evaluate Agent
Evaluate :: Mode

-- | Command Line Arguments
data Args
Args :: String -> String -> String -> String -> String -> String -> String -> String -> String -> String -> String -> String -> Args

-- | See ALG.Algorithm
[algorithm] :: Args -> String

-- | See RPB.ReplayMemory
[memory] :: Args -> String

-- | Circus Server Host Address
[cktHost] :: Args -> String

-- | Circus Server Port
[cktPort] :: Args -> String

-- | ACE ID
[ace] :: Args -> String

-- | ACE PDK
[pdk] :: Args -> String

-- | Design / Action Space
[space] :: Args -> String

-- | (Non-)Goal Environment
[var] :: Args -> String

-- | Checkpoint Base Path
[cpPath] :: Args -> String

-- | MLFlow Server Host Address
[mlfHost] :: Args -> String

-- | MLFlow Server Port
[mlfPort] :: Args -> String

-- | Run Mode
[mode] :: Args -> String

-- | Type Alias for Mini Batch (state, action, reward, state', done)
type MiniBatch = (Tensor, Tensor, Tensor, Tensor, Tensor)

-- | Apply same function to both Left and Right
both :: (a -> b) -> Either a a -> b

-- | Apply same function to both Left and Right and put back into Either
both' :: (a -> b) -> Either a a -> Either b b

-- | Range from 0 to n - 1
range :: Int -> [Int]

-- | First of triple
fst' :: (a, b, c) -> a

-- | Helper function creating split indices
splits :: [Int] -> [[Int]]

-- | First of Triple
fst3 :: (a, b, c) -> a

-- | Uncurry Triple
uncurry3 :: (a -> b -> c -> d) -> (a, b, c) -> d

-- | Like <a>lookup</a> but for a list of keys.
lookup' :: Ord k => [k] -> Map k a -> Maybe [a]

-- | Current Timestamp as formatted string
currentTimeStamp :: String -> IO String

-- | Current Timestamp with default formatting: "%Y%m%d-%H%M%S"
currentTimeStamp' :: IO String

-- | Create a model archive directory for the given algorithm
createModelArchiveDir :: String -> IO String

-- | Create a model archive directory for the given algorithm, ace id and
--   backend
createModelArchiveDir' :: String -> String -> String -> String -> String -> IO String

-- | Optimizer moments at given prefix
saveOptim :: Adam -> FilePath -> IO ()

-- | Load Optimizer State
loadOptim :: Int -> Float -> Float -> FilePath -> IO Adam

-- | Calculate weight Limits based on Layer Dimensions
weightLimit :: Linear -> Float

-- | Type of weight initialization
data Initializer

-- | Normally distributed weights
Normal :: Initializer

-- | Uniformally distributed weights
Uniform :: Initializer

-- | Using T.xavierNormal
XavierNormal :: Initializer

-- | Using T.xavierUniform
XavierUniform :: Initializer

-- | Using T.kaimingNormal
KaimingNormal :: Initializer

-- | Using T.kaimingUniform
KaimingUniform :: Initializer
Dirac :: Initializer
Eye :: Initializer
Ones :: Initializer
Zeros :: Initializer
Constant :: Initializer

-- | Weights for a layer given limits and dimensions.
initWeights :: Initializer -> Float -> Float -> [Int] -> IO IndependentTensor

-- | Initialize Weights of Linear Layer
weightInit :: Initializer -> Float -> Float -> Linear -> IO Linear

-- | Initialize Weights and Bias of Linear Layer
weightInit' :: Initializer -> Float -> Float -> Linear -> IO Linear

-- | Initialize weights uniformally given upper and lower bounds
weightInitUniform :: Float -> Float -> Linear -> IO Linear

-- | Initialize weights uniformally based on Fan In
weightInitUniform' :: Linear -> IO Linear

-- | Initialize weights normally given mean and std bounds
weightInitNormal :: Float -> Float -> Linear -> IO Linear

-- | Initialize weights normally based on Fan In
weightInitNormal' :: Linear -> IO Linear

-- | Softly update parameters from Online Net to Target Net
softUpdate :: Tensor -> Tensor -> Tensor -> Tensor

-- | Softly copy parameters from Online Net to Target Net
softSync :: Parameterized f => Tensor -> f -> f -> IO f

-- | Hard Copy of Parameter from one net to the other
copySync :: Parameterized f => f -> f -> f
instance GHC.Read.Read Lib.Mode
instance GHC.Show.Show Lib.Mode
instance GHC.Classes.Eq Lib.Mode
instance GHC.Show.Show Lib.Args


-- | Circuis REST API Communication
module CKT

-- | Available Circuits
data Circuit

-- | Miller Amplifier (op1)
OP1 :: Circuit

-- | Symmetrical Amplifier (op2)
OP2 :: Circuit

-- | Folded Cascode (op8)
OP8 :: Circuit

-- | Available PDKs
data PDK

-- | X-Fab XH035 350nm Process
XH035 :: PDK

-- | X-Fab XH018 180nm Process
XH018 :: PDK

-- | Available Design / Action Spaces
data Space

-- | Electric Design Space
Electric :: Space

-- | Geometric Design Space
Geometric :: Space

-- | Base Route to Cricus Server
type CircusUrl = String

-- | Generate URL to a Circuit server from meta information
url :: String -> String -> String -> String -> String -> String -> CircusUrl

-- | Get the number of Environments from the given Circus Server instance
numEnvs :: CircusUrl -> IO Int

-- | Get the number of Environments from the given Circus Server instance
actionSpace :: CircusUrl -> IO Int

-- | Get the number of Environments from the given Circus Server instance
observationSpace :: CircusUrl -> IO (Int, Int, Int)

-- | Reset Environments and get (observation, achieved_goal, desired_goal)
reset :: CircusUrl -> IO (Tensor, Tensor, Tensor)

-- | Reset subset of environments with given mask
reset' :: CircusUrl -> Tensor -> IO (Tensor, Tensor, Tensor)

-- | Shorthand for taking a Tensor action and returning Tensors
step :: CircusUrl -> Tensor -> IO (Tensor, Tensor, Tensor, Tensor, Tensor)

-- | Shorthand for getting Tensors
randomAction :: CircusUrl -> IO Tensor

-- | Shorthand for calculating reward given Tensors
calculateReward :: CircusUrl -> Tensor -> Tensor -> IO Tensor

-- | Observation returned from Stepping / Resetting Environment
data Observation a
Observation :: !a -> !a -> !a -> Maybe [Bool] -> Maybe [Float] -> Maybe [Info] -> Observation a

-- | State
[observation] :: Observation a -> !a

-- | Achieved Goal
[achievedGoal] :: Observation a -> !a

-- | Desired Goal
[desiredGoal] :: Observation a -> !a

-- | Terminal Flag
[done] :: Observation a -> Maybe [Bool]

-- | Rewards
[reward] :: Observation a -> Maybe [Float]

-- | Info Dict
[info] :: Observation a -> Maybe [Info]

-- | Info Dict returned from Environment
data Info
Info :: ![String] -> ![String] -> ![String] -> Info

-- | Goal Parameters
[goal] :: Info -> ![String]

-- | Parameters in Action Vector
[inputs] :: Info -> ![String]

-- | Parameters in State Vector
[outputs] :: Info -> ![String]

-- | Action passed to Environment
newtype Action a
Action :: a -> Action a
[action] :: Action a -> a

-- | Shorthand for getting Performance
currentPerformance :: CircusUrl -> IO (Map Int (Map String Float))

-- | Shorthand for getting Goal
currentGoal :: CircusUrl -> IO (Map Int (Map String Float))

-- | Shorthand for getting Sizing
currentSizing :: CircusUrl -> IO (Map Int (Map String Float))

-- | Shorthand for getting last Action
lastAction :: CircusUrl -> IO (Map Int (Map String Float))
instance GHC.Classes.Eq CKT.Circuit
instance GHC.Classes.Eq CKT.PDK
instance GHC.Classes.Eq CKT.Space
instance GHC.Show.Show CKT.Info
instance GHC.Generics.Generic CKT.Info
instance GHC.Show.Show a => GHC.Show.Show (CKT.Action a)
instance GHC.Generics.Generic (CKT.Action a)
instance GHC.Show.Show a => GHC.Show.Show (CKT.Observation a)
instance GHC.Generics.Generic (CKT.Observation a)
instance Data.Aeson.Types.FromJSON.FromJSON (CKT.Observation [[GHC.Types.Float]])
instance Data.Aeson.Types.ToJSON.ToJSON (CKT.Observation [[GHC.Types.Float]])
instance GHC.Base.Functor CKT.Observation
instance Data.Aeson.Types.FromJSON.FromJSON (CKT.Action [[GHC.Types.Float]])
instance Data.Aeson.Types.ToJSON.ToJSON (CKT.Action [[GHC.Types.Float]])
instance GHC.Base.Functor CKT.Action
instance Data.Aeson.Types.FromJSON.FromJSON CKT.Info
instance Data.Aeson.Types.ToJSON.ToJSON CKT.Info
instance GHC.Show.Show CKT.Space
instance GHC.Read.Read CKT.Space
instance GHC.Show.Show CKT.PDK
instance GHC.Read.Read CKT.PDK
instance GHC.Show.Show CKT.Circuit
instance GHC.Read.Read CKT.Circuit

module MLFlow.Extensions

-- | Sanatize JSON for MLFlow: Names may only contain alphanumerics,
--   underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).
sanatizeJSON :: Char -> Char

-- | Data Logging to MLFlow Trackign Server
data Tracker
Tracker :: TrackingURI -> ExperimentID -> String -> Map String RunID -> Tracker

-- | Tracking Server URI
[uri] :: Tracker -> TrackingURI

-- | Experiment ID
[experimentId] :: Tracker -> ExperimentID

-- | Experiment Name
[experimentName] :: Tracker -> String

-- | Run IDs
[runIds] :: Tracker -> Map String RunID

-- | Retrieve a run ID
runId :: Tracker -> String -> RunID

-- | Make new Tracker given a Tracking Server URI
mkTracker :: TrackingURI -> String -> IO Tracker

-- | Make new Tracker given a Hostname and Port
mkTracker' :: String -> Int -> String -> IO Tracker

-- | Create a new Experiment with rng suffix
newExperiment :: Tracker -> String -> IO Tracker

-- | Create a new Experiment
newExperiment' :: Tracker -> String -> IO Tracker

-- | Create a new run with a set of given paramters
newRuns :: Tracker -> [String] -> [Param] -> IO Tracker

-- | New run with algorithm id and #envs as log params
newRuns' :: Int -> Tracker -> IO Tracker

-- | End a run
endRun :: String -> Tracker -> IO Tracker

-- | End all runs of a Tracker
endRuns :: Tracker -> IO Tracker

-- | End all runs and discard tracker
endRuns' :: Tracker -> IO ()

-- | Write Loss to Tracking Server
trackLoss :: Tracker -> Int -> String -> Float -> IO (Response ByteString)

-- | Write Reward to Tracking Server
trackReward :: Tracker -> Int -> Tensor -> IO ()

-- | Clean up a Map returned by Circus Server
sanatizeMap :: Map Int (Map String Float) -> Map Int (Map String Float)

-- | Write Current state of the Environment to Trackign Server
trackEnvState :: Tracker -> CircusUrl -> Int -> IO ()
instance GHC.Show.Show MLFlow.Extensions.Tracker


-- | General Replay Buffer Types and TypeClasses
module ALG

-- | Available Algorithms
data Algorithm

-- | Twin Delayed Deep Deterministic Policy Gradient
TD3 :: Algorithm

-- | Soft Actor Critic
SAC :: Algorithm

-- | Proximal Policy Optimization
PPO :: Algorithm

-- | Replay Buffer Interface
class Agent a

-- | Save an agent at a given Path
saveAgent :: Agent a => FilePath -> a -> IO a

-- | Load an agent saved at a Path
loadAgent :: Agent a => FilePath -> Int -> Int -> Int -> IO a

-- | Take an action to the best Ability
act :: Agent a => a -> Tensor -> IO Tensor

-- | Take a noisy action
act' :: Agent a => a -> Tensor -> IO Tensor

-- | Update Policy
updatePolicy :: Agent a => CircusUrl -> Tracker -> Int -> [MiniBatch] -> a -> IO a
instance GHC.Read.Read ALG.Algorithm
instance GHC.Show.Show ALG.Algorithm
instance GHC.Classes.Eq ALG.Algorithm


-- | Twin Delayed Deep Deterministic Policy Gradient Algorithm Defaults
module ALG.HyperParameters

-- | Print verbose debug output
verbose :: Bool

-- | Number of episodes to play
numEpisodes :: Int

-- | Maximum Number of Steps per Episode
horizonT :: Int

-- | Number of epochs to train
numEpochs :: Int

-- | Mini batch size
batchSize :: Int

-- | Random seed for reproducibility
rngSeed :: Int

-- | Action space lower bound
actionLow :: Float

-- | Action space upper bound
actionHigh :: Float

-- | Policy and Target Update Delay
d :: Int

-- | Noise clipping
c :: Float

-- | Discount Factor
γ :: Tensor

-- | Soft Update coefficient (sometimes "polyak") of the target networks τ
--   ∈ [0,1]
τ :: Tensor

-- | Decay Period
decayPeriod :: Int

-- | Noise Clipping Minimum
σMin :: Float

-- | Noise Clipping Maximum
σMax :: Float

-- | Evaluation Noise standard deviation (σ~)
σEval :: Tensor

-- | Action Noise standard deviation
σAct :: Tensor
σClip :: Float

-- | Number of units per hidden layer
hidDim :: Int

-- | Initial weights
wInit :: Float

-- | Actor Learning Rate
ηφ :: Tensor

-- | Critic Learning Rate
ηθ :: Tensor

-- | ADAM Hyper Parameter β1
β1 :: Float

-- | ADAM Hyper Parameter β2
β2 :: Float

-- | Leaky ReLU Slope
negativeSlope :: Float

-- | Replay Buffer Size
bufferSize :: Int

-- | Initial sample collecting period
warmupPeriode :: Int

-- | Number of Additional Targets to sample
k :: Int


-- | General Replay Buffer Types and TypeClasses
module RPB

-- | Available Replay Buffer Types
data ReplayMemory

-- | Vanilla Replay Buffer
RPB :: ReplayMemory

-- | Prioritized Experience Replay
PER :: ReplayMemory

-- | PPO Style replay Memory
MEM :: ReplayMemory

-- | Emphasizing Recent Experience
ERE :: ReplayMemory

-- | Hindsight Experience Replay
HER :: ReplayMemory

-- | Replay Buffer Interface
class ReplayBuffer b

-- | Return size of current buffer
size :: ReplayBuffer b => b -> Int

-- | Push one buffer into another
push :: ReplayBuffer b => Int -> b -> b -> b

-- | Look Up given list if indices
lookUp :: ReplayBuffer b => [Int] -> b -> b

-- | Take n Random Samples
sampleIO :: ReplayBuffer b => Int -> b -> IO b

-- | Return the Tuple: (s, a, r, s', d) for training
asTuple :: ReplayBuffer b => b -> (Tensor, Tensor, Tensor, Tensor, Tensor)

-- | Collect Experiences in Buffer
collectExperience :: (ReplayBuffer b, Agent a) => CircusUrl -> Tracker -> Int -> a -> IO b

-- | Generate a list of uniformly sampled minibatches
randomBatches :: ReplayBuffer b => Int -> Int -> b -> IO [MiniBatch]

-- | Vanilla Replay Buffer
data Buffer a
Buffer :: !a -> !a -> !a -> !a -> !a -> Buffer a

-- | States
[states] :: Buffer a -> !a

-- | Actions
[actions] :: Buffer a -> !a

-- | Rewards
[rewards] :: Buffer a -> !a

-- | Next States
[states'] :: Buffer a -> !a

-- | Terminal Mask
[dones] :: Buffer a -> !a

-- | Create a new, empty Buffer on the CPU
empty :: Buffer Tensor

-- | How many Trajectories are currently stored in memory
size' :: Buffer Tensor -> Int

-- | Drop number of entries from the beginning of the Buffer
drop :: Int -> Buffer Tensor -> Buffer Tensor

-- | Push one buffer into another one
push' :: Int -> Buffer Tensor -> Buffer Tensor -> Buffer Tensor

-- | Get the given indices from Buffer
lookUp' :: [Int] -> Buffer Tensor -> Buffer Tensor

-- | Take n random samples from Buffer
sampleIO' :: Int -> Buffer Tensor -> IO (Buffer Tensor)

-- | Return (State, Action, Reward, Next State, Done) Tuple
asTuple' :: Buffer Tensor -> MiniBatch

-- | Evaluate Policy for T steps and return experience Buffer
collectStep :: Agent a => CircusUrl -> Tracker -> Int -> Int -> a -> Tensor -> Buffer Tensor -> IO (Buffer Tensor)

-- | Collect experience for a given number of steps
collectExperience' :: Agent a => CircusUrl -> Tracker -> Int -> a -> IO (Buffer Tensor)
instance GHC.Read.Read RPB.ReplayMemory
instance GHC.Show.Show RPB.ReplayMemory
instance GHC.Classes.Eq RPB.ReplayMemory
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.Buffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.Buffer a)
instance GHC.Base.Functor RPB.Buffer
instance GHC.Base.Applicative RPB.Buffer
instance RPB.ReplayBuffer (RPB.Buffer Torch.Tensor.Tensor)


-- | Hindsight Experience Replay
module RPB.HER

-- | Hindsight Experience Replay Strategies for choosing Goals
data Strategy

-- | Only Final States are additional targets
Final :: Strategy

-- | Replay with <a>k</a> random states encountered so far (basically
--   vanilla)
Random :: Strategy

-- | Replay with <a>k</a> random states from same episode.
Episode :: Strategy

-- | Replay with <a>k</a> random states from same episode, that were
--   observed after
Future :: Strategy

-- | Strict Simple/Naive Replay Buffer
data Buffer a
Buffer :: !a -> !a -> !a -> !a -> !a -> !a -> !a -> Buffer a

-- | States
[states] :: Buffer a -> !a

-- | Actions
[actions] :: Buffer a -> !a

-- | Rewards
[rewards] :: Buffer a -> !a

-- | Next States
[states'] :: Buffer a -> !a

-- | Terminal Mask
[dones] :: Buffer a -> !a

-- | Desired Goal
[goals] :: Buffer a -> !a

-- | Acheived Goal
[goals'] :: Buffer a -> !a

-- | Construct an empty HER Buffer
empty :: Buffer Tensor

-- | Split buffer collected from pool by env
envSplit :: Int -> Buffer Tensor -> [Buffer Tensor]

-- | Split a buffer into episodes, dropping the last unfinished
epsSplit :: Buffer Tensor -> [Buffer Tensor]

-- | Sample Additional Goals according to Strategy (drop first).
--   <a>Random</a> is basically the same as <a>Episode</a> you just have to
--   give it the entire buffer, not just the episode.
sampleGoals :: CircusUrl -> Strategy -> Int -> Buffer Tensor -> IO (Buffer Tensor)
instance GHC.Classes.Eq RPB.HER.Strategy
instance GHC.Show.Show RPB.HER.Strategy
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.HER.Buffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.HER.Buffer a)
instance GHC.Base.Functor RPB.HER.Buffer
instance GHC.Base.Applicative RPB.HER.Buffer
instance RPB.ReplayBuffer (RPB.HER.Buffer Torch.Tensor.Tensor)


-- | Twin Delayed Deep Deterministic Policy Gradient Algorithm
module ALG.TD3

-- | Actor Network Architecture
data PolicyNet
PolicyNet :: Linear -> Linear -> Linear -> PolicyNet
[pLayer0] :: PolicyNet -> Linear
[pLayer1] :: PolicyNet -> Linear
[pLayer2] :: PolicyNet -> Linear

-- | Critic Network Architecture
data CriticNet
CriticNet :: Linear -> Linear -> Linear -> Linear -> Linear -> Linear -> CriticNet
[q1Layer0] :: CriticNet -> Linear
[q1Layer1] :: CriticNet -> Linear
[q1Layer2] :: CriticNet -> Linear
[q2Layer0] :: CriticNet -> Linear
[q2Layer1] :: CriticNet -> Linear
[q2Layer2] :: CriticNet -> Linear

-- | TD3 Agent
data Agent
Agent :: PolicyNet -> PolicyNet -> CriticNet -> CriticNet -> Adam -> Adam -> Agent

-- | Online Policy φ
[φ] :: Agent -> PolicyNet

-- | Target Policy φ'
[φ'] :: Agent -> PolicyNet

-- | Online Critic θ
[θ] :: Agent -> CriticNet

-- | Target Critic θ
[θ'] :: Agent -> CriticNet

-- | Policy Optimizer
[φOptim] :: Agent -> Adam

-- | Critic Optimizer
[θOptim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent
instance GHC.Classes.Eq ALG.TD3.PolicyNetSpec
instance GHC.Show.Show ALG.TD3.PolicyNetSpec
instance GHC.Classes.Eq ALG.TD3.CriticNetSpec
instance GHC.Show.Show ALG.TD3.CriticNetSpec
instance Torch.NN.Parameterized ALG.TD3.PolicyNet
instance GHC.Show.Show ALG.TD3.PolicyNet
instance GHC.Generics.Generic ALG.TD3.PolicyNet
instance Torch.NN.Parameterized ALG.TD3.CriticNet
instance GHC.Show.Show ALG.TD3.CriticNet
instance GHC.Generics.Generic ALG.TD3.CriticNet
instance GHC.Show.Show ALG.TD3.Agent
instance GHC.Generics.Generic ALG.TD3.Agent
instance ALG.Agent ALG.TD3.Agent
instance Torch.NN.Randomizable ALG.TD3.CriticNetSpec ALG.TD3.CriticNet
instance Torch.NN.Randomizable ALG.TD3.PolicyNetSpec ALG.TD3.PolicyNet


-- | Artificial Circuit Designer
module ACiD

-- | Runs training on given Agent with Buffer
train :: (Agent a, ReplayBuffer b) => CircusUrl -> Tracker -> String -> Int -> b -> a -> IO ()

-- | Create Agent and Buffer, then run training
run' :: CircusUrl -> Tracker -> String -> Mode -> Algorithm -> ReplayMemory -> IO ()

-- | Clown School
run :: Args -> IO ()
